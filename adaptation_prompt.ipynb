{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptation Prompt Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_PROJECT\"]=\"prompt_learning_methods\"\n",
    "from enum import Enum\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, set_seed\n",
    "from peft import get_peft_config, get_peft_model, AdaptionPromptConfig, TaskType, PeftType\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "device = \"cuda\"\n",
    "model_name_or_path = \"meta-llama/Llama-2-7b-hf\"\n",
    "tokenizer_name_or_path = \"meta-llama/Llama-2-7b-hf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset_name = \"jaykin01/advertisement-copy\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "template = \"\"\"{% for message in messages %}\\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% if loop.last and add_generation_prompt %}{{'<|im_start|>assistant\\n' }}{% endif %}{% endfor %}\"\"\"\n",
    "tokenizer.chat_template = template\n",
    "\n",
    "system_prompt = \"\"\"Create a text ad given the following product and description.\"\"\"\n",
    "\n",
    "def preprocess(samples):\n",
    "    batch = []\n",
    "    for product, desc, ad_copy in zip(samples[\"product\"],samples[\"description\"],samples[\"ad\"]):\n",
    "        conversation = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": f\"\"\"Product: {product}\\nDescription: {desc}\\n\"\"\"},\n",
    "            {\"role\": \"assistant\", \"content\": f\"\"\"Ad: {ad_copy}\\n\"\"\"},\n",
    "        ]\n",
    "        batch.append(tokenizer.apply_chat_template(conversation, tokenize=False))\n",
    "    return {\"content\": batch}\n",
    "\n",
    "dataset = load_dataset(dataset_name)\n",
    "dataset = dataset.map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names\n",
    ")\n",
    "\n",
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[\"train\"].train_test_split(0.1)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the PEFT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptation Prompt Tuning config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = AdaptionPromptConfig(adapter_len=32, \n",
    "                                   adapter_layers=30, \n",
    "                                   task_type=TaskType.CAUSAL_LM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatmlSpecialTokens(str, Enum):\n",
    "    user = \"<|im_start|>user\"\n",
    "    assistant = \"<|im_start|>assistant\"\n",
    "    system = \"<|im_start|>system\"\n",
    "    eos_token = \"<|im_end|>\"\n",
    "    bos_token = \"<s>\"\n",
    "    pad_token = \"<pad>\"\n",
    "\n",
    "    @classmethod\n",
    "    def list(cls):\n",
    "        return [c.value for c in cls]\n",
    "response_template = \"<|im_start|>assistant\\n\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        pad_token=ChatmlSpecialTokens.pad_token.value,\n",
    "        bos_token=ChatmlSpecialTokens.bos_token.value,\n",
    "        eos_token=ChatmlSpecialTokens.eos_token.value,\n",
    "        additional_special_tokens=ChatmlSpecialTokens.list(),\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "tokenizer.chat_template = template\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
